{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OzCAtkdzICXA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/logistic_regression_dataset.xlsx\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-YFU7t-IbQE",
        "outputId": "89ac4d24-9a5c-415a-d483-cc9a0b2ecf26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Feature_1  Feature_2  Feature_3  Output\n",
            "0     0.496714  -0.138264   0.647689       1\n",
            "1     1.523030  -0.234153  -0.234137       1\n",
            "2     1.579213   0.767435  -0.469474       1\n",
            "3     0.542560  -0.463418  -0.465730       1\n",
            "4     0.241962  -1.913280  -1.724918       1\n",
            "..         ...        ...        ...     ...\n",
            "495   0.932591  -1.418366  -1.760809       1\n",
            "496  -1.525656   1.262584  -0.551858       0\n",
            "497   2.558199  -0.564248   0.184551       1\n",
            "498   1.542110   2.006093   2.061504       1\n",
            "499   1.208366   1.024063   0.592527       1\n",
            "\n",
            "[500 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.to_numpy()\n",
        "import numpy as np\n",
        "# Create an empty array to store the normalized data\n",
        "normalized_data = np.zeros_like(dataset)\n",
        "\n",
        "# determine the number of columns\n",
        "noOfColumns = dataset.shape[1]\n",
        "\n",
        "# Normalize one column at a time using aloop\n",
        "for i in range(noOfColumns):\n",
        "    min_val = np.min(dataset[:, i])\n",
        "    max_val = np.max(dataset[:, i])\n",
        "    normalized_data[:, i] = (dataset[:, i] - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "# print normalized data\n",
        "print(normalized_data)\n",
        "dataset=normalized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZX4bSgfI9CA",
        "outputId": "df0a946c-0344-4f26-c221-bb65315b150e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.61961128 0.49096998 0.52172633 1.        ]\n",
            " [0.80703318 0.47579802 0.39013561 1.        ]\n",
            " [0.8172931  0.63427343 0.3550173  1.        ]\n",
            " ...\n",
            " [0.99607192 0.42356914 0.45261451 1.        ]\n",
            " [0.81051752 0.83025905 0.73270338 1.        ]\n",
            " [0.7495705  0.67487815 0.51349482 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=dataset[:,0:3]\n",
        "print(X)\n",
        "Y=dataset[:,3]\n",
        "print(Y.shape)\n",
        "Y=Y.reshape(Y.shape[0],1)\n",
        "print(Y.shape)\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEe3dIkKJDoH",
        "outputId": "00a55392-bd61-4237-adf0-61b7408ff587"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.61961128 0.49096998 0.52172633]\n",
            " [0.80703318 0.47579802 0.39013561]\n",
            " [0.8172931  0.63427343 0.3550173 ]\n",
            " ...\n",
            " [0.99607192 0.42356914 0.45261451]\n",
            " [0.81051752 0.83025905 0.73270338]\n",
            " [0.7495705  0.67487815 0.51349482]]\n",
            "(500,)\n",
            "(500, 1)\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + tf.exp(-z))\n",
        "\n",
        "def lossFunction(yPred, yReal):\n",
        "    n = yReal.shape[0]\n",
        "    return -tf.reduce_mean(yReal * tf.math.log(yPred) + (1 - yReal) * tf.math.log(1 - yPred))\n"
      ],
      "metadata": {
        "id": "xFKOLFlLJVU4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = tf.Variable(X)\n",
        "Y_tensor = tf.Variable(Y)\n",
        "\n",
        "print(f'X_tensor shape = {X_tensor.shape}')\n",
        "print(f'Y_tensor shape = {Y_tensor.shape}')\n",
        "\n",
        "W_tensor = tf.Variable(np.random.randn(3, 1), dtype=tf.double)\n",
        "print(f'W_tensor shape = {W_tensor.shape}')\n",
        "b_tensor = tf.Variable(np.random.randn(1,1), dtype=tf.double)\n",
        "print(f'b_tensor shape = {b_tensor.shape}')\n",
        "learning_rate = 0.001\n",
        "epochs=1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGkVXZoZLKEj",
        "outputId": "52365624-6158-4961-c92f-3b345b44c73e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_tensor shape = (500, 3)\n",
            "Y_tensor shape = (500, 1)\n",
            "W_tensor shape = (3, 1)\n",
            "b_tensor shape = (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "      Z = tf.matmul(X_tensor,W_tensor) + b_tensor\n",
        "      YPred = sigmoid(Z)\n",
        "      loss = lossFunction(YPred,Y_tensor)\n",
        "\n",
        "  dlossBydW = tape.gradient(loss,W_tensor)\n",
        "  dlossBydb = tape.gradient(loss,b_tensor)\n",
        "\n",
        "\n",
        "  W_tensor.assign_sub(learning_rate*dlossBydW)\n",
        "  b_tensor.assign_sub(learning_rate*dlossBydb)\n",
        "  print(f\"Epoch {i+1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "print(W_tensor.numpy())\n",
        "print(b_tensor.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDibzX1uLZC_",
        "outputId": "0bf7333a-5aa1-4204-f519-aea47f6c960c",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6979448627237279\n",
            "Epoch 2, Loss: 0.6979361361481634\n",
            "Epoch 3, Loss: 0.6979274134851752\n",
            "Epoch 4, Loss: 0.6979186947314554\n",
            "Epoch 5, Loss: 0.6979099798836996\n",
            "Epoch 6, Loss: 0.6979012689386056\n",
            "Epoch 7, Loss: 0.6978925618928745\n",
            "Epoch 8, Loss: 0.6978838587432098\n",
            "Epoch 9, Loss: 0.6978751594863176\n",
            "Epoch 10, Loss: 0.6978664641189071\n",
            "Epoch 11, Loss: 0.6978577726376901\n",
            "Epoch 12, Loss: 0.6978490850393814\n",
            "Epoch 13, Loss: 0.6978404013206982\n",
            "Epoch 14, Loss: 0.6978317214783607\n",
            "Epoch 15, Loss: 0.6978230455090918\n",
            "Epoch 16, Loss: 0.6978143734096168\n",
            "Epoch 17, Loss: 0.6978057051766646\n",
            "Epoch 18, Loss: 0.6977970408069661\n",
            "Epoch 19, Loss: 0.697788380297255\n",
            "Epoch 20, Loss: 0.6977797236442682\n",
            "Epoch 21, Loss: 0.6977710708447449\n",
            "Epoch 22, Loss: 0.697762421895427\n",
            "Epoch 23, Loss: 0.6977537767930596\n",
            "Epoch 24, Loss: 0.6977451355343901\n",
            "Epoch 25, Loss: 0.6977364981161688\n",
            "Epoch 26, Loss: 0.6977278645351488\n",
            "Epoch 27, Loss: 0.6977192347880857\n",
            "Epoch 28, Loss: 0.6977106088717381\n",
            "Epoch 29, Loss: 0.697701986782867\n",
            "Epoch 30, Loss: 0.6976933685182364\n",
            "Epoch 31, Loss: 0.6976847540746131\n",
            "Epoch 32, Loss: 0.6976761434487662\n",
            "Epoch 33, Loss: 0.6976675366374676\n",
            "Epoch 34, Loss: 0.6976589336374924\n",
            "Epoch 35, Loss: 0.6976503344456179\n",
            "Epoch 36, Loss: 0.6976417390586243\n",
            "Epoch 37, Loss: 0.6976331474732943\n",
            "Epoch 38, Loss: 0.6976245596864138\n",
            "Epoch 39, Loss: 0.6976159756947709\n",
            "Epoch 40, Loss: 0.6976073954951566\n",
            "Epoch 41, Loss: 0.6975988190843645\n",
            "Epoch 42, Loss: 0.6975902464591911\n",
            "Epoch 43, Loss: 0.6975816776164354\n",
            "Epoch 44, Loss: 0.6975731125528991\n",
            "Epoch 45, Loss: 0.6975645512653866\n",
            "Epoch 46, Loss: 0.6975559937507052\n",
            "Epoch 47, Loss: 0.6975474400056646\n",
            "Epoch 48, Loss: 0.697538890027077\n",
            "Epoch 49, Loss: 0.697530343811758\n",
            "Epoch 50, Loss: 0.6975218013565254\n",
            "Epoch 51, Loss: 0.6975132626581992\n",
            "Epoch 52, Loss: 0.6975047277136032\n",
            "Epoch 53, Loss: 0.697496196519563\n",
            "Epoch 54, Loss: 0.6974876690729069\n",
            "Epoch 55, Loss: 0.6974791453704665\n",
            "Epoch 56, Loss: 0.6974706254090753\n",
            "Epoch 57, Loss: 0.6974621091855701\n",
            "Epoch 58, Loss: 0.6974535966967897\n",
            "Epoch 59, Loss: 0.6974450879395763\n",
            "Epoch 60, Loss: 0.6974365829107741\n",
            "Epoch 61, Loss: 0.6974280816072305\n",
            "Epoch 62, Loss: 0.697419584025795\n",
            "Epoch 63, Loss: 0.69741109016332\n",
            "Epoch 64, Loss: 0.6974026000166609\n",
            "Epoch 65, Loss: 0.6973941135826753\n",
            "Epoch 66, Loss: 0.6973856308582235\n",
            "Epoch 67, Loss: 0.6973771518401685\n",
            "Epoch 68, Loss: 0.697368676525376\n",
            "Epoch 69, Loss: 0.697360204910714\n",
            "Epoch 70, Loss: 0.697351736993054\n",
            "Epoch 71, Loss: 0.6973432727692689\n",
            "Epoch 72, Loss: 0.6973348122362352\n",
            "Epoch 73, Loss: 0.6973263553908318\n",
            "Epoch 74, Loss: 0.6973179022299398\n",
            "Epoch 75, Loss: 0.6973094527504435\n",
            "Epoch 76, Loss: 0.6973010069492293\n",
            "Epoch 77, Loss: 0.6972925648231868\n",
            "Epoch 78, Loss: 0.697284126369208\n",
            "Epoch 79, Loss: 0.6972756915841869\n",
            "Epoch 80, Loss: 0.697267260465021\n",
            "Epoch 81, Loss: 0.6972588330086098\n",
            "Epoch 82, Loss: 0.6972504092118559\n",
            "Epoch 83, Loss: 0.6972419890716642\n",
            "Epoch 84, Loss: 0.6972335725849422\n",
            "Epoch 85, Loss: 0.6972251597485999\n",
            "Epoch 86, Loss: 0.6972167505595505\n",
            "Epoch 87, Loss: 0.6972083450147089\n",
            "Epoch 88, Loss: 0.6971999431109932\n",
            "Epoch 89, Loss: 0.6971915448453239\n",
            "Epoch 90, Loss: 0.6971831502146243\n",
            "Epoch 91, Loss: 0.6971747592158202\n",
            "Epoch 92, Loss: 0.6971663718458394\n",
            "Epoch 93, Loss: 0.6971579881016133\n",
            "Epoch 94, Loss: 0.6971496079800753\n",
            "Epoch 95, Loss: 0.6971412314781612\n",
            "Epoch 96, Loss: 0.6971328585928099\n",
            "Epoch 97, Loss: 0.6971244893209627\n",
            "Epoch 98, Loss: 0.6971161236595631\n",
            "Epoch 99, Loss: 0.6971077616055577\n",
            "Epoch 100, Loss: 0.6970994031558951\n",
            "Epoch 101, Loss: 0.6970910483075274\n",
            "Epoch 102, Loss: 0.697082697057408\n",
            "Epoch 103, Loss: 0.6970743494024941\n",
            "Epoch 104, Loss: 0.6970660053397444\n",
            "Epoch 105, Loss: 0.6970576648661211\n",
            "Epoch 106, Loss: 0.6970493279785882\n",
            "Epoch 107, Loss: 0.6970409946741126\n",
            "Epoch 108, Loss: 0.697032664949664\n",
            "Epoch 109, Loss: 0.697024338802214\n",
            "Epoch 110, Loss: 0.6970160162287375\n",
            "Epoch 111, Loss: 0.6970076972262114\n",
            "Epoch 112, Loss: 0.6969993817916152\n",
            "Epoch 113, Loss: 0.6969910699219312\n",
            "Epoch 114, Loss: 0.6969827616141442\n",
            "Epoch 115, Loss: 0.6969744568652413\n",
            "Epoch 116, Loss: 0.6969661556722122\n",
            "Epoch 117, Loss: 0.6969578580320495\n",
            "Epoch 118, Loss: 0.6969495639417477\n",
            "Epoch 119, Loss: 0.6969412733983045\n",
            "Epoch 120, Loss: 0.6969329863987198\n",
            "Epoch 121, Loss: 0.6969247029399958\n",
            "Epoch 122, Loss: 0.6969164230191376\n",
            "Epoch 123, Loss: 0.6969081466331527\n",
            "Epoch 124, Loss: 0.696899873779051\n",
            "Epoch 125, Loss: 0.6968916044538452\n",
            "Epoch 126, Loss: 0.6968833386545504\n",
            "Epoch 127, Loss: 0.696875076378184\n",
            "Epoch 128, Loss: 0.6968668176217663\n",
            "Epoch 129, Loss: 0.6968585623823194\n",
            "Epoch 130, Loss: 0.696850310656869\n",
            "Epoch 131, Loss: 0.6968420624424422\n",
            "Epoch 132, Loss: 0.6968338177360692\n",
            "Epoch 133, Loss: 0.696825576534783\n",
            "Epoch 134, Loss: 0.6968173388356184\n",
            "Epoch 135, Loss: 0.6968091046356129\n",
            "Epoch 136, Loss: 0.6968008739318068\n",
            "Epoch 137, Loss: 0.6967926467212429\n",
            "Epoch 138, Loss: 0.6967844230009658\n",
            "Epoch 139, Loss: 0.6967762027680233\n",
            "Epoch 140, Loss: 0.6967679860194657\n",
            "Epoch 141, Loss: 0.6967597727523452\n",
            "Epoch 142, Loss: 0.6967515629637172\n",
            "Epoch 143, Loss: 0.6967433566506391\n",
            "Epoch 144, Loss: 0.6967351538101708\n",
            "Epoch 145, Loss: 0.6967269544393748\n",
            "Epoch 146, Loss: 0.6967187585353163\n",
            "Epoch 147, Loss: 0.6967105660950624\n",
            "Epoch 148, Loss: 0.6967023771156833\n",
            "Epoch 149, Loss: 0.6966941915942512\n",
            "Epoch 150, Loss: 0.696686009527841\n",
            "Epoch 151, Loss: 0.6966778309135302\n",
            "Epoch 152, Loss: 0.6966696557483983\n",
            "Epoch 153, Loss: 0.6966614840295278\n",
            "Epoch 154, Loss: 0.696653315754003\n",
            "Epoch 155, Loss: 0.6966451509189117\n",
            "Epoch 156, Loss: 0.696636989521343\n",
            "Epoch 157, Loss: 0.6966288315583892\n",
            "Epoch 158, Loss: 0.6966206770271447\n",
            "Epoch 159, Loss: 0.6966125259247067\n",
            "Epoch 160, Loss: 0.6966043782481743\n",
            "Epoch 161, Loss: 0.6965962339946498\n",
            "Epoch 162, Loss: 0.6965880931612374\n",
            "Epoch 163, Loss: 0.6965799557450436\n",
            "Epoch 164, Loss: 0.6965718217431777\n",
            "Epoch 165, Loss: 0.6965636911527515\n",
            "Epoch 166, Loss: 0.6965555639708791\n",
            "Epoch 167, Loss: 0.696547440194677\n",
            "Epoch 168, Loss: 0.6965393198212639\n",
            "Epoch 169, Loss: 0.6965312028477614\n",
            "Epoch 170, Loss: 0.6965230892712932\n",
            "Epoch 171, Loss: 0.6965149790889856\n",
            "Epoch 172, Loss: 0.6965068722979677\n",
            "Epoch 173, Loss: 0.6964987688953698\n",
            "Epoch 174, Loss: 0.6964906688783258\n",
            "Epoch 175, Loss: 0.6964825722439716\n",
            "Epoch 176, Loss: 0.6964744789894455\n",
            "Epoch 177, Loss: 0.6964663891118883\n",
            "Epoch 178, Loss: 0.6964583026084433\n",
            "Epoch 179, Loss: 0.6964502194762558\n",
            "Epoch 180, Loss: 0.696442139712474\n",
            "Epoch 181, Loss: 0.6964340633142485\n",
            "Epoch 182, Loss: 0.6964259902787318\n",
            "Epoch 183, Loss: 0.6964179206030789\n",
            "Epoch 184, Loss: 0.6964098542844479\n",
            "Epoch 185, Loss: 0.6964017913199986\n",
            "Epoch 186, Loss: 0.6963937317068936\n",
            "Epoch 187, Loss: 0.696385675442297\n",
            "Epoch 188, Loss: 0.6963776225233769\n",
            "Epoch 189, Loss: 0.6963695729473024\n",
            "Epoch 190, Loss: 0.6963615267112456\n",
            "Epoch 191, Loss: 0.6963534838123808\n",
            "Epoch 192, Loss: 0.6963454442478849\n",
            "Epoch 193, Loss: 0.6963374080149369\n",
            "Epoch 194, Loss: 0.6963293751107185\n",
            "Epoch 195, Loss: 0.6963213455324134\n",
            "Epoch 196, Loss: 0.696313319277208\n",
            "Epoch 197, Loss: 0.6963052963422909\n",
            "Epoch 198, Loss: 0.696297276724853\n",
            "Epoch 199, Loss: 0.696289260422088\n",
            "Epoch 200, Loss: 0.6962812474311916\n",
            "Epoch 201, Loss: 0.6962732377493617\n",
            "Epoch 202, Loss: 0.6962652313737991\n",
            "Epoch 203, Loss: 0.6962572283017067\n",
            "Epoch 204, Loss: 0.6962492285302893\n",
            "Epoch 205, Loss: 0.6962412320567548\n",
            "Epoch 206, Loss: 0.6962332388783133\n",
            "Epoch 207, Loss: 0.6962252489921771\n",
            "Epoch 208, Loss: 0.6962172623955605\n",
            "Epoch 209, Loss: 0.6962092790856809\n",
            "Epoch 210, Loss: 0.6962012990597574\n",
            "Epoch 211, Loss: 0.696193322315012\n",
            "Epoch 212, Loss: 0.6961853488486686\n",
            "Epoch 213, Loss: 0.6961773786579538\n",
            "Epoch 214, Loss: 0.6961694117400961\n",
            "Epoch 215, Loss: 0.6961614480923265\n",
            "Epoch 216, Loss: 0.6961534877118789\n",
            "Epoch 217, Loss: 0.6961455305959889\n",
            "Epoch 218, Loss: 0.6961375767418945\n",
            "Epoch 219, Loss: 0.6961296261468363\n",
            "Epoch 220, Loss: 0.6961216788080569\n",
            "Epoch 221, Loss: 0.6961137347228016\n",
            "Epoch 222, Loss: 0.6961057938883175\n",
            "Epoch 223, Loss: 0.6960978563018551\n",
            "Epoch 224, Loss: 0.6960899219606657\n",
            "Epoch 225, Loss: 0.6960819908620042\n",
            "Epoch 226, Loss: 0.6960740630031271\n",
            "Epoch 227, Loss: 0.6960661383812935\n",
            "Epoch 228, Loss: 0.696058216993765\n",
            "Epoch 229, Loss: 0.696050298837805\n",
            "Epoch 230, Loss: 0.6960423839106795\n",
            "Epoch 231, Loss: 0.6960344722096572\n",
            "Epoch 232, Loss: 0.6960265637320086\n",
            "Epoch 233, Loss: 0.6960186584750061\n",
            "Epoch 234, Loss: 0.6960107564359255\n",
            "Epoch 235, Loss: 0.6960028576120442\n",
            "Epoch 236, Loss: 0.695994962000642\n",
            "Epoch 237, Loss: 0.6959870695990011\n",
            "Epoch 238, Loss: 0.695979180404406\n",
            "Epoch 239, Loss: 0.6959712944141434\n",
            "Epoch 240, Loss: 0.6959634116255021\n",
            "Epoch 241, Loss: 0.6959555320357738\n",
            "Epoch 242, Loss: 0.6959476556422517\n",
            "Epoch 243, Loss: 0.6959397824422323\n",
            "Epoch 244, Loss: 0.6959319124330132\n",
            "Epoch 245, Loss: 0.6959240456118952\n",
            "Epoch 246, Loss: 0.6959161819761811\n",
            "Epoch 247, Loss: 0.6959083215231758\n",
            "Epoch 248, Loss: 0.6959004642501868\n",
            "Epoch 249, Loss: 0.6958926101545235\n",
            "Epoch 250, Loss: 0.6958847592334976\n",
            "Epoch 251, Loss: 0.6958769114844239\n",
            "Epoch 252, Loss: 0.6958690669046184\n",
            "Epoch 253, Loss: 0.6958612254913996\n",
            "Epoch 254, Loss: 0.6958533872420889\n",
            "Epoch 255, Loss: 0.6958455521540092\n",
            "Epoch 256, Loss: 0.6958377202244862\n",
            "Epoch 257, Loss: 0.6958298914508477\n",
            "Epoch 258, Loss: 0.6958220658304236\n",
            "Epoch 259, Loss: 0.6958142433605462\n",
            "Epoch 260, Loss: 0.6958064240385501\n",
            "Epoch 261, Loss: 0.695798607861772\n",
            "Epoch 262, Loss: 0.695790794827551\n",
            "Epoch 263, Loss: 0.6957829849332283\n",
            "Epoch 264, Loss: 0.6957751781761478\n",
            "Epoch 265, Loss: 0.6957673745536548\n",
            "Epoch 266, Loss: 0.6957595740630977\n",
            "Epoch 267, Loss: 0.6957517767018268\n",
            "Epoch 268, Loss: 0.6957439824671944\n",
            "Epoch 269, Loss: 0.6957361913565553\n",
            "Epoch 270, Loss: 0.6957284033672667\n",
            "Epoch 271, Loss: 0.695720618496688\n",
            "Epoch 272, Loss: 0.6957128367421801\n",
            "Epoch 273, Loss: 0.6957050581011073\n",
            "Epoch 274, Loss: 0.6956972825708353\n",
            "Epoch 275, Loss: 0.6956895101487323\n",
            "Epoch 276, Loss: 0.6956817408321686\n",
            "Epoch 277, Loss: 0.695673974618517\n",
            "Epoch 278, Loss: 0.6956662115051524\n",
            "Epoch 279, Loss: 0.6956584514894519\n",
            "Epoch 280, Loss: 0.6956506945687946\n",
            "Epoch 281, Loss: 0.6956429407405621\n",
            "Epoch 282, Loss: 0.6956351900021385\n",
            "Epoch 283, Loss: 0.695627442350909\n",
            "Epoch 284, Loss: 0.6956196977842625\n",
            "Epoch 285, Loss: 0.695611956299589\n",
            "Epoch 286, Loss: 0.6956042178942813\n",
            "Epoch 287, Loss: 0.695596482565734\n",
            "Epoch 288, Loss: 0.6955887503113443\n",
            "Epoch 289, Loss: 0.6955810211285113\n",
            "Epoch 290, Loss: 0.6955732950146365\n",
            "Epoch 291, Loss: 0.6955655719671233\n",
            "Epoch 292, Loss: 0.6955578519833779\n",
            "Epoch 293, Loss: 0.695550135060808\n",
            "Epoch 294, Loss: 0.695542421196824\n",
            "Epoch 295, Loss: 0.695534710388838\n",
            "Epoch 296, Loss: 0.6955270026342649\n",
            "Epoch 297, Loss: 0.6955192979305214\n",
            "Epoch 298, Loss: 0.6955115962750266\n",
            "Epoch 299, Loss: 0.6955038976652013\n",
            "Epoch 300, Loss: 0.6954962020984693\n",
            "Epoch 301, Loss: 0.6954885095722558\n",
            "Epoch 302, Loss: 0.6954808200839888\n",
            "Epoch 303, Loss: 0.6954731336310978\n",
            "Epoch 304, Loss: 0.6954654502110155\n",
            "Epoch 305, Loss: 0.6954577698211754\n",
            "Epoch 306, Loss: 0.6954500924590142\n",
            "Epoch 307, Loss: 0.6954424181219706\n",
            "Epoch 308, Loss: 0.6954347468074854\n",
            "Epoch 309, Loss: 0.6954270785130016\n",
            "Epoch 310, Loss: 0.695419413235964\n",
            "Epoch 311, Loss: 0.69541175097382\n",
            "Epoch 312, Loss: 0.6954040917240191\n",
            "Epoch 313, Loss: 0.6953964354840129\n",
            "Epoch 314, Loss: 0.695388782251255\n",
            "Epoch 315, Loss: 0.6953811320232013\n",
            "Epoch 316, Loss: 0.6953734847973101\n",
            "Epoch 317, Loss: 0.6953658405710414\n",
            "Epoch 318, Loss: 0.6953581993418578\n",
            "Epoch 319, Loss: 0.6953505611072235\n",
            "Epoch 320, Loss: 0.6953429258646054\n",
            "Epoch 321, Loss: 0.6953352936114725\n",
            "Epoch 322, Loss: 0.6953276643452954\n",
            "Epoch 323, Loss: 0.6953200380635475\n",
            "Epoch 324, Loss: 0.6953124147637038\n",
            "Epoch 325, Loss: 0.6953047944432417\n",
            "Epoch 326, Loss: 0.695297177099641\n",
            "Epoch 327, Loss: 0.6952895627303832\n",
            "Epoch 328, Loss: 0.6952819513329521\n",
            "Epoch 329, Loss: 0.6952743429048337\n",
            "Epoch 330, Loss: 0.6952667374435161\n",
            "Epoch 331, Loss: 0.6952591349464893\n",
            "Epoch 332, Loss: 0.6952515354112458\n",
            "Epoch 333, Loss: 0.6952439388352801\n",
            "Epoch 334, Loss: 0.6952363452160888\n",
            "Epoch 335, Loss: 0.6952287545511704\n",
            "Epoch 336, Loss: 0.6952211668380259\n",
            "Epoch 337, Loss: 0.6952135820741583\n",
            "Epoch 338, Loss: 0.6952060002570725\n",
            "Epoch 339, Loss: 0.6951984213842757\n",
            "Epoch 340, Loss: 0.6951908454532773\n",
            "Epoch 341, Loss: 0.6951832724615887\n",
            "Epoch 342, Loss: 0.6951757024067234\n",
            "Epoch 343, Loss: 0.6951681352861968\n",
            "Epoch 344, Loss: 0.6951605710975272\n",
            "Epoch 345, Loss: 0.6951530098382338\n",
            "Epoch 346, Loss: 0.6951454515058391\n",
            "Epoch 347, Loss: 0.6951378960978667\n",
            "Epoch 348, Loss: 0.695130343611843\n",
            "Epoch 349, Loss: 0.6951227940452962\n",
            "Epoch 350, Loss: 0.6951152473957568\n",
            "Epoch 351, Loss: 0.6951077036607569\n",
            "Epoch 352, Loss: 0.6951001628378313\n",
            "Epoch 353, Loss: 0.6950926249245165\n",
            "Epoch 354, Loss: 0.6950850899183513\n",
            "Epoch 355, Loss: 0.6950775578168764\n",
            "Epoch 356, Loss: 0.695070028617635\n",
            "Epoch 357, Loss: 0.6950625023181715\n",
            "Epoch 358, Loss: 0.6950549789160336\n",
            "Epoch 359, Loss: 0.6950474584087701\n",
            "Epoch 360, Loss: 0.6950399407939325\n",
            "Epoch 361, Loss: 0.6950324260690736\n",
            "Epoch 362, Loss: 0.6950249142317493\n",
            "Epoch 363, Loss: 0.6950174052795166\n",
            "Epoch 364, Loss: 0.6950098992099354\n",
            "Epoch 365, Loss: 0.6950023960205672\n",
            "Epoch 366, Loss: 0.6949948957089758\n",
            "Epoch 367, Loss: 0.6949873982727265\n",
            "Epoch 368, Loss: 0.6949799037093873\n",
            "Epoch 369, Loss: 0.6949724120165286\n",
            "Epoch 370, Loss: 0.6949649231917214\n",
            "Epoch 371, Loss: 0.6949574372325406\n",
            "Epoch 372, Loss: 0.6949499541365616\n",
            "Epoch 373, Loss: 0.694942473901363\n",
            "Epoch 374, Loss: 0.6949349965245243\n",
            "Epoch 375, Loss: 0.6949275220036285\n",
            "Epoch 376, Loss: 0.6949200503362595\n",
            "Epoch 377, Loss: 0.6949125815200036\n",
            "Epoch 378, Loss: 0.694905115552449\n",
            "Epoch 379, Loss: 0.6948976524311866\n",
            "Epoch 380, Loss: 0.6948901921538088\n",
            "Epoch 381, Loss: 0.6948827347179096\n",
            "Epoch 382, Loss: 0.694875280121086\n",
            "Epoch 383, Loss: 0.6948678283609366\n",
            "Epoch 384, Loss: 0.694860379435062\n",
            "Epoch 385, Loss: 0.6948529333410647\n",
            "Epoch 386, Loss: 0.6948454900765496\n",
            "Epoch 387, Loss: 0.6948380496391233\n",
            "Epoch 388, Loss: 0.6948306120263947\n",
            "Epoch 389, Loss: 0.6948231772359748\n",
            "Epoch 390, Loss: 0.6948157452654761\n",
            "Epoch 391, Loss: 0.6948083161125135\n",
            "Epoch 392, Loss: 0.694800889774704\n",
            "Epoch 393, Loss: 0.6947934662496666\n",
            "Epoch 394, Loss: 0.694786045535022\n",
            "Epoch 395, Loss: 0.6947786276283936\n",
            "Epoch 396, Loss: 0.6947712125274059\n",
            "Epoch 397, Loss: 0.6947638002296862\n",
            "Epoch 398, Loss: 0.6947563907328634\n",
            "Epoch 399, Loss: 0.6947489840345686\n",
            "Epoch 400, Loss: 0.6947415801324347\n",
            "Epoch 401, Loss: 0.6947341790240971\n",
            "Epoch 402, Loss: 0.6947267807071923\n",
            "Epoch 403, Loss: 0.6947193851793599\n",
            "Epoch 404, Loss: 0.6947119924382408\n",
            "Epoch 405, Loss: 0.6947046024814781\n",
            "Epoch 406, Loss: 0.6946972153067168\n",
            "Epoch 407, Loss: 0.6946898309116039\n",
            "Epoch 408, Loss: 0.694682449293789\n",
            "Epoch 409, Loss: 0.6946750704509225\n",
            "Epoch 410, Loss: 0.6946676943806579\n",
            "Epoch 411, Loss: 0.6946603210806501\n",
            "Epoch 412, Loss: 0.6946529505485565\n",
            "Epoch 413, Loss: 0.6946455827820357\n",
            "Epoch 414, Loss: 0.694638217778749\n",
            "Epoch 415, Loss: 0.6946308555363595\n",
            "Epoch 416, Loss: 0.694623496052532\n",
            "Epoch 417, Loss: 0.6946161393249338\n",
            "Epoch 418, Loss: 0.6946087853512338\n",
            "Epoch 419, Loss: 0.6946014341291027\n",
            "Epoch 420, Loss: 0.6945940856562138\n",
            "Epoch 421, Loss: 0.6945867399302419\n",
            "Epoch 422, Loss: 0.6945793969488638\n",
            "Epoch 423, Loss: 0.6945720567097587\n",
            "Epoch 424, Loss: 0.6945647192106071\n",
            "Epoch 425, Loss: 0.6945573844490923\n",
            "Epoch 426, Loss: 0.6945500524228987\n",
            "Epoch 427, Loss: 0.6945427231297131\n",
            "Epoch 428, Loss: 0.6945353965672243\n",
            "Epoch 429, Loss: 0.694528072733123\n",
            "Epoch 430, Loss: 0.6945207516251019\n",
            "Epoch 431, Loss: 0.6945134332408558\n",
            "Epoch 432, Loss: 0.6945061175780807\n",
            "Epoch 433, Loss: 0.6944988046344759\n",
            "Epoch 434, Loss: 0.6944914944077415\n",
            "Epoch 435, Loss: 0.6944841868955799\n",
            "Epoch 436, Loss: 0.6944768820956958\n",
            "Epoch 437, Loss: 0.6944695800057953\n",
            "Epoch 438, Loss: 0.6944622806235868\n",
            "Epoch 439, Loss: 0.6944549839467806\n",
            "Epoch 440, Loss: 0.6944476899730889\n",
            "Epoch 441, Loss: 0.694440398700226\n",
            "Epoch 442, Loss: 0.6944331101259077\n",
            "Epoch 443, Loss: 0.6944258242478524\n",
            "Epoch 444, Loss: 0.6944185410637796\n",
            "Epoch 445, Loss: 0.694411260571412\n",
            "Epoch 446, Loss: 0.6944039827684726\n",
            "Epoch 447, Loss: 0.6943967076526878\n",
            "Epoch 448, Loss: 0.6943894352217851\n",
            "Epoch 449, Loss: 0.6943821654734942\n",
            "Epoch 450, Loss: 0.6943748984055468\n",
            "Epoch 451, Loss: 0.6943676340156762\n",
            "Epoch 452, Loss: 0.6943603723016182\n",
            "Epoch 453, Loss: 0.6943531132611098\n",
            "Epoch 454, Loss: 0.6943458568918907\n",
            "Epoch 455, Loss: 0.6943386031917019\n",
            "Epoch 456, Loss: 0.6943313521582866\n",
            "Epoch 457, Loss: 0.6943241037893899\n",
            "Epoch 458, Loss: 0.6943168580827587\n",
            "Epoch 459, Loss: 0.6943096150361422\n",
            "Epoch 460, Loss: 0.6943023746472909\n",
            "Epoch 461, Loss: 0.6942951369139576\n",
            "Epoch 462, Loss: 0.6942879018338972\n",
            "Epoch 463, Loss: 0.6942806694048661\n",
            "Epoch 464, Loss: 0.6942734396246228\n",
            "Epoch 465, Loss: 0.6942662124909275\n",
            "Epoch 466, Loss: 0.6942589880015428\n",
            "Epoch 467, Loss: 0.6942517661542329\n",
            "Epoch 468, Loss: 0.6942445469467636\n",
            "Epoch 469, Loss: 0.6942373303769033\n",
            "Epoch 470, Loss: 0.6942301164424214\n",
            "Epoch 471, Loss: 0.69422290514109\n",
            "Epoch 472, Loss: 0.6942156964706828\n",
            "Epoch 473, Loss: 0.6942084904289755\n",
            "Epoch 474, Loss: 0.6942012870137453\n",
            "Epoch 475, Loss: 0.6941940862227716\n",
            "Epoch 476, Loss: 0.694186888053836\n",
            "Epoch 477, Loss: 0.6941796925047214\n",
            "Epoch 478, Loss: 0.6941724995732129\n",
            "Epoch 479, Loss: 0.6941653092570974\n",
            "Epoch 480, Loss: 0.6941581215541637\n",
            "Epoch 481, Loss: 0.6941509364622027\n",
            "Epoch 482, Loss: 0.6941437539790067\n",
            "Epoch 483, Loss: 0.6941365741023703\n",
            "Epoch 484, Loss: 0.6941293968300897\n",
            "Epoch 485, Loss: 0.6941222221599633\n",
            "Epoch 486, Loss: 0.6941150500897914\n",
            "Epoch 487, Loss: 0.6941078806173753\n",
            "Epoch 488, Loss: 0.6941007137405193\n",
            "Epoch 489, Loss: 0.694093549457029\n",
            "Epoch 490, Loss: 0.6940863877647121\n",
            "Epoch 491, Loss: 0.6940792286613778\n",
            "Epoch 492, Loss: 0.6940720721448377\n",
            "Epoch 493, Loss: 0.6940649182129046\n",
            "Epoch 494, Loss: 0.6940577668633939\n",
            "Epoch 495, Loss: 0.6940506180941223\n",
            "Epoch 496, Loss: 0.6940434719029083\n",
            "Epoch 497, Loss: 0.6940363282875731\n",
            "Epoch 498, Loss: 0.6940291872459389\n",
            "Epoch 499, Loss: 0.6940220487758297\n",
            "Epoch 500, Loss: 0.6940149128750722\n",
            "Epoch 501, Loss: 0.6940077795414941\n",
            "Epoch 502, Loss: 0.6940006487729253\n",
            "Epoch 503, Loss: 0.6939935205671978\n",
            "Epoch 504, Loss: 0.6939863949221448\n",
            "Epoch 505, Loss: 0.6939792718356019\n",
            "Epoch 506, Loss: 0.6939721513054065\n",
            "Epoch 507, Loss: 0.6939650333293973\n",
            "Epoch 508, Loss: 0.6939579179054156\n",
            "Epoch 509, Loss: 0.6939508050313039\n",
            "Epoch 510, Loss: 0.6939436947049075\n",
            "Epoch 511, Loss: 0.6939365869240721\n",
            "Epoch 512, Loss: 0.6939294816866461\n",
            "Epoch 513, Loss: 0.6939223789904801\n",
            "Epoch 514, Loss: 0.6939152788334257\n",
            "Epoch 515, Loss: 0.6939081812133367\n",
            "Epoch 516, Loss: 0.6939010861280688\n",
            "Epoch 517, Loss: 0.6938939935754794\n",
            "Epoch 518, Loss: 0.6938869035534279\n",
            "Epoch 519, Loss: 0.6938798160597753\n",
            "Epoch 520, Loss: 0.6938727310923843\n",
            "Epoch 521, Loss: 0.6938656486491201\n",
            "Epoch 522, Loss: 0.6938585687278491\n",
            "Epoch 523, Loss: 0.6938514913264396\n",
            "Epoch 524, Loss: 0.6938444164427617\n",
            "Epoch 525, Loss: 0.6938373440746877\n",
            "Epoch 526, Loss: 0.693830274220091\n",
            "Epoch 527, Loss: 0.6938232068768477\n",
            "Epoch 528, Loss: 0.6938161420428351\n",
            "Epoch 529, Loss: 0.6938090797159324\n",
            "Epoch 530, Loss: 0.6938020198940207\n",
            "Epoch 531, Loss: 0.693794962574983\n",
            "Epoch 532, Loss: 0.6937879077567036\n",
            "Epoch 533, Loss: 0.6937808554370694\n",
            "Epoch 534, Loss: 0.6937738056139683\n",
            "Epoch 535, Loss: 0.6937667582852909\n",
            "Epoch 536, Loss: 0.6937597134489286\n",
            "Epoch 537, Loss: 0.6937526711027755\n",
            "Epoch 538, Loss: 0.6937456312447268\n",
            "Epoch 539, Loss: 0.6937385938726798\n",
            "Epoch 540, Loss: 0.6937315589845334\n",
            "Epoch 541, Loss: 0.693724526578189\n",
            "Epoch 542, Loss: 0.6937174966515487\n",
            "Epoch 543, Loss: 0.6937104692025173\n",
            "Epoch 544, Loss: 0.6937034442290007\n",
            "Epoch 545, Loss: 0.6936964217289071\n",
            "Epoch 546, Loss: 0.6936894017001464\n",
            "Epoch 547, Loss: 0.6936823841406299\n",
            "Epoch 548, Loss: 0.6936753690482712\n",
            "Epoch 549, Loss: 0.6936683564209853\n",
            "Epoch 550, Loss: 0.6936613462566893\n",
            "Epoch 551, Loss: 0.6936543385533016\n",
            "Epoch 552, Loss: 0.6936473333087427\n",
            "Epoch 553, Loss: 0.6936403305209351\n",
            "Epoch 554, Loss: 0.6936333301878028\n",
            "Epoch 555, Loss: 0.6936263323072712\n",
            "Epoch 556, Loss: 0.6936193368772685\n",
            "Epoch 557, Loss: 0.6936123438957232\n",
            "Epoch 558, Loss: 0.6936053533605671\n",
            "Epoch 559, Loss: 0.6935983652697325\n",
            "Epoch 560, Loss: 0.6935913796211545\n",
            "Epoch 561, Loss: 0.6935843964127691\n",
            "Epoch 562, Loss: 0.6935774156425148\n",
            "Epoch 563, Loss: 0.6935704373083312\n",
            "Epoch 564, Loss: 0.6935634614081602\n",
            "Epoch 565, Loss: 0.6935564879399448\n",
            "Epoch 566, Loss: 0.6935495169016306\n",
            "Epoch 567, Loss: 0.6935425482911642\n",
            "Epoch 568, Loss: 0.6935355821064947\n",
            "Epoch 569, Loss: 0.693528618345572\n",
            "Epoch 570, Loss: 0.6935216570063486\n",
            "Epoch 571, Loss: 0.6935146980867783\n",
            "Epoch 572, Loss: 0.6935077415848169\n",
            "Epoch 573, Loss: 0.6935007874984217\n",
            "Epoch 574, Loss: 0.6934938358255516\n",
            "Epoch 575, Loss: 0.693486886564168\n",
            "Epoch 576, Loss: 0.6934799397122332\n",
            "Epoch 577, Loss: 0.6934729952677117\n",
            "Epoch 578, Loss: 0.6934660532285696\n",
            "Epoch 579, Loss: 0.6934591135927745\n",
            "Epoch 580, Loss: 0.6934521763582964\n",
            "Epoch 581, Loss: 0.6934452415231063\n",
            "Epoch 582, Loss: 0.6934383090851772\n",
            "Epoch 583, Loss: 0.6934313790424842\n",
            "Epoch 584, Loss: 0.6934244513930037\n",
            "Epoch 585, Loss: 0.6934175261347136\n",
            "Epoch 586, Loss: 0.6934106032655943\n",
            "Epoch 587, Loss: 0.6934036827836271\n",
            "Epoch 588, Loss: 0.6933967646867959\n",
            "Epoch 589, Loss: 0.6933898489730852\n",
            "Epoch 590, Loss: 0.6933829356404824\n",
            "Epoch 591, Loss: 0.6933760246869756\n",
            "Epoch 592, Loss: 0.6933691161105555\n",
            "Epoch 593, Loss: 0.6933622099092138\n",
            "Epoch 594, Loss: 0.6933553060809443\n",
            "Epoch 595, Loss: 0.6933484046237425\n",
            "Epoch 596, Loss: 0.6933415055356055\n",
            "Epoch 597, Loss: 0.693334608814532\n",
            "Epoch 598, Loss: 0.6933277144585227\n",
            "Epoch 599, Loss: 0.6933208224655798\n",
            "Epoch 600, Loss: 0.6933139328337075\n",
            "Epoch 601, Loss: 0.6933070455609109\n",
            "Epoch 602, Loss: 0.6933001606451981\n",
            "Epoch 603, Loss: 0.6932932780845776\n",
            "Epoch 604, Loss: 0.6932863978770604\n",
            "Epoch 605, Loss: 0.693279520020659\n",
            "Epoch 606, Loss: 0.6932726445133877\n",
            "Epoch 607, Loss: 0.6932657713532621\n",
            "Epoch 608, Loss: 0.6932589005382997\n",
            "Epoch 609, Loss: 0.6932520320665202\n",
            "Epoch 610, Loss: 0.6932451659359441\n",
            "Epoch 611, Loss: 0.6932383021445943\n",
            "Epoch 612, Loss: 0.6932314406904949\n",
            "Epoch 613, Loss: 0.6932245815716722\n",
            "Epoch 614, Loss: 0.6932177247861536\n",
            "Epoch 615, Loss: 0.6932108703319688\n",
            "Epoch 616, Loss: 0.6932040182071487\n",
            "Epoch 617, Loss: 0.6931971684097259\n",
            "Epoch 618, Loss: 0.6931903209377351\n",
            "Epoch 619, Loss: 0.6931834757892124\n",
            "Epoch 620, Loss: 0.6931766329621953\n",
            "Epoch 621, Loss: 0.6931697924547234\n",
            "Epoch 622, Loss: 0.6931629542648381\n",
            "Epoch 623, Loss: 0.6931561183905818\n",
            "Epoch 624, Loss: 0.6931492848299993\n",
            "Epoch 625, Loss: 0.6931424535811366\n",
            "Epoch 626, Loss: 0.6931356246420414\n",
            "Epoch 627, Loss: 0.6931287980107637\n",
            "Epoch 628, Loss: 0.6931219736853541\n",
            "Epoch 629, Loss: 0.6931151516638656\n",
            "Epoch 630, Loss: 0.6931083319443527\n",
            "Epoch 631, Loss: 0.6931015145248716\n",
            "Epoch 632, Loss: 0.6930946994034802\n",
            "Epoch 633, Loss: 0.6930878865782376\n",
            "Epoch 634, Loss: 0.6930810760472055\n",
            "Epoch 635, Loss: 0.693074267808446\n",
            "Epoch 636, Loss: 0.6930674618600242\n",
            "Epoch 637, Loss: 0.6930606582000058\n",
            "Epoch 638, Loss: 0.6930538568264585\n",
            "Epoch 639, Loss: 0.6930470577374519\n",
            "Epoch 640, Loss: 0.6930402609310572\n",
            "Epoch 641, Loss: 0.6930334664053469\n",
            "Epoch 642, Loss: 0.693026674158395\n",
            "Epoch 643, Loss: 0.6930198841882781\n",
            "Epoch 644, Loss: 0.6930130964930734\n",
            "Epoch 645, Loss: 0.6930063110708604\n",
            "Epoch 646, Loss: 0.6929995279197201\n",
            "Epoch 647, Loss: 0.6929927470377347\n",
            "Epoch 648, Loss: 0.6929859684229887\n",
            "Epoch 649, Loss: 0.692979192073568\n",
            "Epoch 650, Loss: 0.6929724179875595\n",
            "Epoch 651, Loss: 0.6929656461630532\n",
            "Epoch 652, Loss: 0.6929588765981392\n",
            "Epoch 653, Loss: 0.6929521092909099\n",
            "Epoch 654, Loss: 0.6929453442394596\n",
            "Epoch 655, Loss: 0.6929385814418838\n",
            "Epoch 656, Loss: 0.6929318208962798\n",
            "Epoch 657, Loss: 0.6929250626007463\n",
            "Epoch 658, Loss: 0.6929183065533839\n",
            "Epoch 659, Loss: 0.692911552752295\n",
            "Epoch 660, Loss: 0.6929048011955828\n",
            "Epoch 661, Loss: 0.6928980518813533\n",
            "Epoch 662, Loss: 0.6928913048077131\n",
            "Epoch 663, Loss: 0.6928845599727712\n",
            "Epoch 664, Loss: 0.6928778173746373\n",
            "Epoch 665, Loss: 0.6928710770114238\n",
            "Epoch 666, Loss: 0.6928643388812438\n",
            "Epoch 667, Loss: 0.6928576029822123\n",
            "Epoch 668, Loss: 0.6928508693124465\n",
            "Epoch 669, Loss: 0.6928441378700643\n",
            "Epoch 670, Loss: 0.6928374086531855\n",
            "Epoch 671, Loss: 0.6928306816599322\n",
            "Epoch 672, Loss: 0.6928239568884268\n",
            "Epoch 673, Loss: 0.6928172343367945\n",
            "Epoch 674, Loss: 0.6928105140031616\n",
            "Epoch 675, Loss: 0.6928037958856558\n",
            "Epoch 676, Loss: 0.692797079982407\n",
            "Epoch 677, Loss: 0.692790366291546\n",
            "Epoch 678, Loss: 0.6927836548112056\n",
            "Epoch 679, Loss: 0.6927769455395205\n",
            "Epoch 680, Loss: 0.6927702384746262\n",
            "Epoch 681, Loss: 0.6927635336146604\n",
            "Epoch 682, Loss: 0.6927568309577621\n",
            "Epoch 683, Loss: 0.6927501305020721\n",
            "Epoch 684, Loss: 0.6927434322457329\n",
            "Epoch 685, Loss: 0.6927367361868879\n",
            "Epoch 686, Loss: 0.692730042323683\n",
            "Epoch 687, Loss: 0.6927233506542652\n",
            "Epoch 688, Loss: 0.692716661176783\n",
            "Epoch 689, Loss: 0.6927099738893867\n",
            "Epoch 690, Loss: 0.6927032887902282\n",
            "Epoch 691, Loss: 0.6926966058774608\n",
            "Epoch 692, Loss: 0.6926899251492395\n",
            "Epoch 693, Loss: 0.6926832466037209\n",
            "Epoch 694, Loss: 0.6926765702390633\n",
            "Epoch 695, Loss: 0.6926698960534259\n",
            "Epoch 696, Loss: 0.6926632240449703\n",
            "Epoch 697, Loss: 0.6926565542118596\n",
            "Epoch 698, Loss: 0.6926498865522578\n",
            "Epoch 699, Loss: 0.6926432210643311\n",
            "Epoch 700, Loss: 0.692636557746247\n",
            "Epoch 701, Loss: 0.6926298965961748\n",
            "Epoch 702, Loss: 0.6926232376122851\n",
            "Epoch 703, Loss: 0.6926165807927501\n",
            "Epoch 704, Loss: 0.6926099261357438\n",
            "Epoch 705, Loss: 0.6926032736394412\n",
            "Epoch 706, Loss: 0.6925966233020199\n",
            "Epoch 707, Loss: 0.692589975121658\n",
            "Epoch 708, Loss: 0.6925833290965354\n",
            "Epoch 709, Loss: 0.6925766852248343\n",
            "Epoch 710, Loss: 0.6925700435047374\n",
            "Epoch 711, Loss: 0.6925634039344295\n",
            "Epoch 712, Loss: 0.6925567665120973\n",
            "Epoch 713, Loss: 0.6925501312359282\n",
            "Epoch 714, Loss: 0.6925434981041119\n",
            "Epoch 715, Loss: 0.6925368671148392\n",
            "Epoch 716, Loss: 0.6925302382663024\n",
            "Epoch 717, Loss: 0.692523611556696\n",
            "Epoch 718, Loss: 0.6925169869842153\n",
            "Epoch 719, Loss: 0.6925103645470575\n",
            "Epoch 720, Loss: 0.6925037442434213\n",
            "Epoch 721, Loss: 0.692497126071507\n",
            "Epoch 722, Loss: 0.6924905100295162\n",
            "Epoch 723, Loss: 0.6924838961156524\n",
            "Epoch 724, Loss: 0.6924772843281203\n",
            "Epoch 725, Loss: 0.6924706746651264\n",
            "Epoch 726, Loss: 0.6924640671248784\n",
            "Epoch 727, Loss: 0.692457461705586\n",
            "Epoch 728, Loss: 0.69245085840546\n",
            "Epoch 729, Loss: 0.6924442572227129\n",
            "Epoch 730, Loss: 0.6924376581555589\n",
            "Epoch 731, Loss: 0.6924310612022136\n",
            "Epoch 732, Loss: 0.692424466360894\n",
            "Epoch 733, Loss: 0.6924178736298185\n",
            "Epoch 734, Loss: 0.6924112830072078\n",
            "Epoch 735, Loss: 0.6924046944912831\n",
            "Epoch 736, Loss: 0.6923981080802679\n",
            "Epoch 737, Loss: 0.6923915237723866\n",
            "Epoch 738, Loss: 0.6923849415658658\n",
            "Epoch 739, Loss: 0.6923783614589332\n",
            "Epoch 740, Loss: 0.6923717834498178\n",
            "Epoch 741, Loss: 0.6923652075367507\n",
            "Epoch 742, Loss: 0.6923586337179638\n",
            "Epoch 743, Loss: 0.6923520619916917\n",
            "Epoch 744, Loss: 0.6923454923561689\n",
            "Epoch 745, Loss: 0.6923389248096328\n",
            "Epoch 746, Loss: 0.6923323593503217\n",
            "Epoch 747, Loss: 0.6923257959764748\n",
            "Epoch 748, Loss: 0.6923192346863345\n",
            "Epoch 749, Loss: 0.6923126754781433\n",
            "Epoch 750, Loss: 0.6923061183501453\n",
            "Epoch 751, Loss: 0.6922995633005864\n",
            "Epoch 752, Loss: 0.6922930103277145\n",
            "Epoch 753, Loss: 0.692286459429778\n",
            "Epoch 754, Loss: 0.6922799106050276\n",
            "Epoch 755, Loss: 0.692273363851715\n",
            "Epoch 756, Loss: 0.6922668191680936\n",
            "Epoch 757, Loss: 0.6922602765524184\n",
            "Epoch 758, Loss: 0.6922537360029457\n",
            "Epoch 759, Loss: 0.6922471975179336\n",
            "Epoch 760, Loss: 0.6922406610956412\n",
            "Epoch 761, Loss: 0.6922341267343294\n",
            "Epoch 762, Loss: 0.6922275944322605\n",
            "Epoch 763, Loss: 0.6922210641876986\n",
            "Epoch 764, Loss: 0.6922145359989088\n",
            "Epoch 765, Loss: 0.692208009864158\n",
            "Epoch 766, Loss: 0.6922014857817145\n",
            "Epoch 767, Loss: 0.6921949637498481\n",
            "Epoch 768, Loss: 0.69218844376683\n",
            "Epoch 769, Loss: 0.692181925830933\n",
            "Epoch 770, Loss: 0.6921754099404313\n",
            "Epoch 771, Loss: 0.6921688960936005\n",
            "Epoch 772, Loss: 0.6921623842887179\n",
            "Epoch 773, Loss: 0.6921558745240625\n",
            "Epoch 774, Loss: 0.6921493667979137\n",
            "Epoch 775, Loss: 0.6921428611085537\n",
            "Epoch 776, Loss: 0.6921363574542652\n",
            "Epoch 777, Loss: 0.6921298558333331\n",
            "Epoch 778, Loss: 0.6921233562440432\n",
            "Epoch 779, Loss: 0.6921168586846832\n",
            "Epoch 780, Loss: 0.6921103631535418\n",
            "Epoch 781, Loss: 0.6921038696489097\n",
            "Epoch 782, Loss: 0.6920973781690785\n",
            "Epoch 783, Loss: 0.6920908887123416\n",
            "Epoch 784, Loss: 0.692084401276994\n",
            "Epoch 785, Loss: 0.6920779158613318\n",
            "Epoch 786, Loss: 0.6920714324636529\n",
            "Epoch 787, Loss: 0.6920649510822566\n",
            "Epoch 788, Loss: 0.6920584717154432\n",
            "Epoch 789, Loss: 0.6920519943615151\n",
            "Epoch 790, Loss: 0.6920455190187758\n",
            "Epoch 791, Loss: 0.6920390456855302\n",
            "Epoch 792, Loss: 0.6920325743600849\n",
            "Epoch 793, Loss: 0.6920261050407478\n",
            "Epoch 794, Loss: 0.6920196377258285\n",
            "Epoch 795, Loss: 0.6920131724136375\n",
            "Epoch 796, Loss: 0.6920067091024868\n",
            "Epoch 797, Loss: 0.692000247790691\n",
            "Epoch 798, Loss: 0.6919937884765648\n",
            "Epoch 799, Loss: 0.6919873311584245\n",
            "Epoch 800, Loss: 0.6919808758345889\n",
            "Epoch 801, Loss: 0.6919744225033766\n",
            "Epoch 802, Loss: 0.6919679711631093\n",
            "Epoch 803, Loss: 0.6919615218121089\n",
            "Epoch 804, Loss: 0.6919550744486996\n",
            "Epoch 805, Loss: 0.6919486290712064\n",
            "Epoch 806, Loss: 0.691942185677956\n",
            "Epoch 807, Loss: 0.6919357442672766\n",
            "Epoch 808, Loss: 0.6919293048374977\n",
            "Epoch 809, Loss: 0.6919228673869504\n",
            "Epoch 810, Loss: 0.6919164319139672\n",
            "Epoch 811, Loss: 0.6919099984168816\n",
            "Epoch 812, Loss: 0.6919035668940294\n",
            "Epoch 813, Loss: 0.6918971373437468\n",
            "Epoch 814, Loss: 0.6918907097643721\n",
            "Epoch 815, Loss: 0.6918842841542451\n",
            "Epoch 816, Loss: 0.6918778605117064\n",
            "Epoch 817, Loss: 0.6918714388350988\n",
            "Epoch 818, Loss: 0.691865019122766\n",
            "Epoch 819, Loss: 0.691858601373053\n",
            "Epoch 820, Loss: 0.6918521855843067\n",
            "Epoch 821, Loss: 0.6918457717548753\n",
            "Epoch 822, Loss: 0.6918393598831081\n",
            "Epoch 823, Loss: 0.691832949967356\n",
            "Epoch 824, Loss: 0.6918265420059716\n",
            "Epoch 825, Loss: 0.6918201359973085\n",
            "Epoch 826, Loss: 0.6918137319397217\n",
            "Epoch 827, Loss: 0.691807329831568\n",
            "Epoch 828, Loss: 0.6918009296712053\n",
            "Epoch 829, Loss: 0.6917945314569931\n",
            "Epoch 830, Loss: 0.6917881351872918\n",
            "Epoch 831, Loss: 0.6917817408604642\n",
            "Epoch 832, Loss: 0.6917753484748734\n",
            "Epoch 833, Loss: 0.6917689580288847\n",
            "Epoch 834, Loss: 0.6917625695208646\n",
            "Epoch 835, Loss: 0.6917561829491806\n",
            "Epoch 836, Loss: 0.6917497983122021\n",
            "Epoch 837, Loss: 0.6917434156082998\n",
            "Epoch 838, Loss: 0.6917370348358455\n",
            "Epoch 839, Loss: 0.6917306559932128\n",
            "Epoch 840, Loss: 0.6917242790787764\n",
            "Epoch 841, Loss: 0.6917179040909127\n",
            "Epoch 842, Loss: 0.691711531027999\n",
            "Epoch 843, Loss: 0.6917051598884144\n",
            "Epoch 844, Loss: 0.6916987906705393\n",
            "Epoch 845, Loss: 0.6916924233727555\n",
            "Epoch 846, Loss: 0.6916860579934462\n",
            "Epoch 847, Loss: 0.6916796945309958\n",
            "Epoch 848, Loss: 0.6916733329837905\n",
            "Epoch 849, Loss: 0.6916669733502174\n",
            "Epoch 850, Loss: 0.691660615628665\n",
            "Epoch 851, Loss: 0.6916542598175238\n",
            "Epoch 852, Loss: 0.6916479059151852\n",
            "Epoch 853, Loss: 0.6916415539200417\n",
            "Epoch 854, Loss: 0.6916352038304878\n",
            "Epoch 855, Loss: 0.6916288556449193\n",
            "Epoch 856, Loss: 0.6916225093617326\n",
            "Epoch 857, Loss: 0.6916161649793268\n",
            "Epoch 858, Loss: 0.6916098224961008\n",
            "Epoch 859, Loss: 0.6916034819104566\n",
            "Epoch 860, Loss: 0.6915971432207958\n",
            "Epoch 861, Loss: 0.6915908064255228\n",
            "Epoch 862, Loss: 0.6915844715230428\n",
            "Epoch 863, Loss: 0.6915781385117622\n",
            "Epoch 864, Loss: 0.691571807390089\n",
            "Epoch 865, Loss: 0.6915654781564327\n",
            "Epoch 866, Loss: 0.6915591508092035\n",
            "Epoch 867, Loss: 0.6915528253468141\n",
            "Epoch 868, Loss: 0.6915465017676774\n",
            "Epoch 869, Loss: 0.6915401800702085\n",
            "Epoch 870, Loss: 0.6915338602528234\n",
            "Epoch 871, Loss: 0.6915275423139394\n",
            "Epoch 872, Loss: 0.6915212262519758\n",
            "Epoch 873, Loss: 0.6915149120653526\n",
            "Epoch 874, Loss: 0.6915085997524913\n",
            "Epoch 875, Loss: 0.691502289311815\n",
            "Epoch 876, Loss: 0.6914959807417478\n",
            "Epoch 877, Loss: 0.6914896740407152\n",
            "Epoch 878, Loss: 0.6914833692071445\n",
            "Epoch 879, Loss: 0.6914770662394639\n",
            "Epoch 880, Loss: 0.6914707651361032\n",
            "Epoch 881, Loss: 0.6914644658954933\n",
            "Epoch 882, Loss: 0.6914581685160666\n",
            "Epoch 883, Loss: 0.6914518729962568\n",
            "Epoch 884, Loss: 0.6914455793344989\n",
            "Epoch 885, Loss: 0.6914392875292296\n",
            "Epoch 886, Loss: 0.6914329975788863\n",
            "Epoch 887, Loss: 0.6914267094819083\n",
            "Epoch 888, Loss: 0.6914204232367362\n",
            "Epoch 889, Loss: 0.6914141388418115\n",
            "Epoch 890, Loss: 0.6914078562955773\n",
            "Epoch 891, Loss: 0.6914015755964783\n",
            "Epoch 892, Loss: 0.69139529674296\n",
            "Epoch 893, Loss: 0.6913890197334697\n",
            "Epoch 894, Loss: 0.6913827445664562\n",
            "Epoch 895, Loss: 0.6913764712403685\n",
            "Epoch 896, Loss: 0.6913701997536582\n",
            "Epoch 897, Loss: 0.6913639301047777\n",
            "Epoch 898, Loss: 0.691357662292181\n",
            "Epoch 899, Loss: 0.6913513963143227\n",
            "Epoch 900, Loss: 0.6913451321696598\n",
            "Epoch 901, Loss: 0.6913388698566497\n",
            "Epoch 902, Loss: 0.6913326093737516\n",
            "Epoch 903, Loss: 0.6913263507194258\n",
            "Epoch 904, Loss: 0.6913200938921342\n",
            "Epoch 905, Loss: 0.6913138388903398\n",
            "Epoch 906, Loss: 0.6913075857125068\n",
            "Epoch 907, Loss: 0.6913013343571013\n",
            "Epoch 908, Loss: 0.6912950848225898\n",
            "Epoch 909, Loss: 0.691288837107441\n",
            "Epoch 910, Loss: 0.6912825912101246\n",
            "Epoch 911, Loss: 0.691276347129111\n",
            "Epoch 912, Loss: 0.6912701048628731\n",
            "Epoch 913, Loss: 0.6912638644098845\n",
            "Epoch 914, Loss: 0.6912576257686195\n",
            "Epoch 915, Loss: 0.6912513889375547\n",
            "Epoch 916, Loss: 0.6912451539151676\n",
            "Epoch 917, Loss: 0.6912389206999371\n",
            "Epoch 918, Loss: 0.6912326892903433\n",
            "Epoch 919, Loss: 0.6912264596848673\n",
            "Epoch 920, Loss: 0.6912202318819923\n",
            "Epoch 921, Loss: 0.6912140058802021\n",
            "Epoch 922, Loss: 0.6912077816779821\n",
            "Epoch 923, Loss: 0.691201559273819\n",
            "Epoch 924, Loss: 0.6911953386662008\n",
            "Epoch 925, Loss: 0.6911891198536164\n",
            "Epoch 926, Loss: 0.6911829028345567\n",
            "Epoch 927, Loss: 0.6911766876075136\n",
            "Epoch 928, Loss: 0.69117047417098\n",
            "Epoch 929, Loss: 0.6911642625234503\n",
            "Epoch 930, Loss: 0.6911580526634206\n",
            "Epoch 931, Loss: 0.6911518445893876\n",
            "Epoch 932, Loss: 0.6911456382998497\n",
            "Epoch 933, Loss: 0.6911394337933063\n",
            "Epoch 934, Loss: 0.6911332310682587\n",
            "Epoch 935, Loss: 0.6911270301232089\n",
            "Epoch 936, Loss: 0.6911208309566602\n",
            "Epoch 937, Loss: 0.6911146335671176\n",
            "Epoch 938, Loss: 0.6911084379530871\n",
            "Epoch 939, Loss: 0.6911022441130759\n",
            "Epoch 940, Loss: 0.6910960520455928\n",
            "Epoch 941, Loss: 0.6910898617491474\n",
            "Epoch 942, Loss: 0.6910836732222512\n",
            "Epoch 943, Loss: 0.6910774864634163\n",
            "Epoch 944, Loss: 0.6910713014711568\n",
            "Epoch 945, Loss: 0.6910651182439874\n",
            "Epoch 946, Loss: 0.6910589367804246\n",
            "Epoch 947, Loss: 0.691052757078986\n",
            "Epoch 948, Loss: 0.69104657913819\n",
            "Epoch 949, Loss: 0.6910404029565571\n",
            "Epoch 950, Loss: 0.6910342285326088\n",
            "Epoch 951, Loss: 0.6910280558648675\n",
            "Epoch 952, Loss: 0.6910218849518569\n",
            "Epoch 953, Loss: 0.6910157157921026\n",
            "Epoch 954, Loss: 0.691009548384131\n",
            "Epoch 955, Loss: 0.6910033827264697\n",
            "Epoch 956, Loss: 0.6909972188176478\n",
            "Epoch 957, Loss: 0.6909910566561954\n",
            "Epoch 958, Loss: 0.6909848962406441\n",
            "Epoch 959, Loss: 0.6909787375695267\n",
            "Epoch 960, Loss: 0.690972580641377\n",
            "Epoch 961, Loss: 0.6909664254547309\n",
            "Epoch 962, Loss: 0.6909602720081243\n",
            "Epoch 963, Loss: 0.6909541203000954\n",
            "Epoch 964, Loss: 0.6909479703291831\n",
            "Epoch 965, Loss: 0.6909418220939278\n",
            "Epoch 966, Loss: 0.6909356755928713\n",
            "Epoch 967, Loss: 0.6909295308245562\n",
            "Epoch 968, Loss: 0.6909233877875265\n",
            "Epoch 969, Loss: 0.6909172464803278\n",
            "Epoch 970, Loss: 0.6909111069015066\n",
            "Epoch 971, Loss: 0.6909049690496106\n",
            "Epoch 972, Loss: 0.6908988329231892\n",
            "Epoch 973, Loss: 0.6908926985207925\n",
            "Epoch 974, Loss: 0.6908865658409721\n",
            "Epoch 975, Loss: 0.6908804348822809\n",
            "Epoch 976, Loss: 0.6908743056432731\n",
            "Epoch 977, Loss: 0.6908681781225039\n",
            "Epoch 978, Loss: 0.69086205231853\n",
            "Epoch 979, Loss: 0.6908559282299088\n",
            "Epoch 980, Loss: 0.6908498058552\n",
            "Epoch 981, Loss: 0.6908436851929632\n",
            "Epoch 982, Loss: 0.6908375662417605\n",
            "Epoch 983, Loss: 0.6908314490001543\n",
            "Epoch 984, Loss: 0.6908253334667088\n",
            "Epoch 985, Loss: 0.6908192196399893\n",
            "Epoch 986, Loss: 0.6908131075185617\n",
            "Epoch 987, Loss: 0.6908069971009945\n",
            "Epoch 988, Loss: 0.6908008883858561\n",
            "Epoch 989, Loss: 0.690794781371717\n",
            "Epoch 990, Loss: 0.6907886760571482\n",
            "Epoch 991, Loss: 0.6907825724407227\n",
            "Epoch 992, Loss: 0.6907764705210142\n",
            "Epoch 993, Loss: 0.6907703702965978\n",
            "Epoch 994, Loss: 0.6907642717660497\n",
            "Epoch 995, Loss: 0.6907581749279476\n",
            "Epoch 996, Loss: 0.6907520797808703\n",
            "Epoch 997, Loss: 0.6907459863233976\n",
            "Epoch 998, Loss: 0.6907398945541106\n",
            "Epoch 999, Loss: 0.690733804471592\n",
            "Epoch 1000, Loss: 0.6907277160744253\n",
            "[[ 0.3958785 ]\n",
            " [ 0.3885309 ]\n",
            " [-0.49765105]]\n",
            "[[-0.34465329]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X[-200:]\n",
        "Y_test = Y[-200:]\n",
        "\n",
        "# Predict Y using the learned weights and bias\n",
        "Z_test = tf.matmul(X_test, W_tensor) + b_tensor\n",
        "Y_pred_test = sigmoid(Z_test)\n",
        "Y_pred_test = tf.round(Y_pred_test)  # Convert probabilities to 0 or 1\n",
        "\n",
        "# Calculate performance metrics\n",
        "TP = np.sum((Y_pred_test.numpy() == 1) & (Y_test == 1))\n",
        "TN = np.sum((Y_pred_test.numpy() == 0) & (Y_test == 0))\n",
        "FP = np.sum((Y_pred_test.numpy() == 1) & (Y_test == 0))\n",
        "FN = np.sum((Y_pred_test.numpy() == 0) & (Y_test == 1))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Precision\n",
        "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "\n",
        "# Recall\n",
        "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "\n",
        "# F1 Score\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocsmcR7uCmf0",
        "outputId": "de35d58f-e4b8-4d3e-cedf-950b27944e64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.535\n",
            "Precision: 0.631578947368421\n",
            "Recall: 0.12244897959183673\n",
            "F1 Score: 0.20512820512820512\n"
          ]
        }
      ]
    }
  ]
}